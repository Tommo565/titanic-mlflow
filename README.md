# titanic-mlflow-skl

This repository serves as a prototype for Saint Gobain UK's AI and Machine Learning projects implemented in Python using Scikit-Learn and MLFlow. 

The focus isn't the model, but the framework, standards and tools used to structure the project and deploy the model.

## Key Features
* Git feature/branch workflow
* PEP8 code standards, docstrings and code linting with flake8
* Abstraction of complex shell commands with `make` via the Makefile
* Reproducibility via a `conda` environment and associated `environment.yaml`
* Managing development and test configurations via the `env.dev` and `env.test` files.
* Separating the application configuration (`.env`), metadata (`parameters.yaml`) and functionality (`src`).
* Unit tests for functions via `pytest` and unit test coverage via `coverage`
* Logging generated by `loguru`. This needs replacing (see TODO)
* Model experimentation and deployment with MLFlow.
* Machine Learning visualisation produced via `yellowbrick` and `scikitplot`

Over time, I will be expanding upon these topics with documentation, sessions and tutorials etc. to ensure that the "why", "what" and "how" get properly explained.

## Quickstart
1. Ensure that you have the following software installed:
    * [Anaconda](https://docs.anaconda.com/anaconda/install/)
    * [Git](https://git-scm.com/download/win)
    * [Make](https://www.gnu.org/software/make/) / [Make for Git Bash on Windows](https://gist.github.com/evanwill/0207876c3243bbb6863e65ec5dc3f058)

2. [Download the data & configuration files from OneDrive]() and:
* Save the `env.dev` and `env.test` files into the root of the repository.
* Save the `train_test_raw.csv` and `holdout_raw.csv` files in the `data-dev` directory that you downloaded into the `data/dev` directory. This is the live data used to train and test the model.
* Save the `train_test_raw.csv` and `holdout_raw.csv` files in the `data-dummy` directory that you downloaded into the `data/dev` directory. This is created dummy data used to run the tests on the overall application.

3. Create the sqlite databases to store MLFlow experiments as follows:
* `make create db-dev`
* `make create db-test`
This will create two databases in the `db/` directory to store MLFlow experiment data for development and testing respectively.

4. Run `make create-environment` to create the conda environment. This will install the packages listed in the `requirements-conda` and `requirements-pip` directories from `conda` and `pip` respectively and save a full list of package dependencies to `environment.yaml`.

5. To run an experiement run `make run-experiment`. You can update the hyperparameters of the model by changing the appropriate sections in `parameters.yaml`

6. To start the MLFlow dashboard run `make mlflow-server`. This will start a webserver for the UI which you can access via `localhost:5001` in your browser.

7. To stage a model for deployment run `make run-deployment`. This will save the model ready to be deployed in the `artifacts/dev/models` directory. Note that the command will error if a model name already exists so you'll need to either delete the model directory in `artifacts/dev/models` or change the `model_name` parameter in `parameters.yaml`.

8. To start the model deployment server run `make mlflow-serve-model`. This will serve the model at `localhost:1235`. This is best accessed through the `query_model_server.ipynb` notebook which contains some pre-baked code to generate a prediction from it.

9. (Optional) To install the environment as a Jupyter kernel run `make create-kernel`. You shouldn't need this to run the notebook in Step 8 however.

10. (Optional) To run the tests run `make tests`. This will run the tests in the `tests` directory using `pytest` and output a coverage report alongside them using `coverage`.

11. (Optional) To install extra packages, update one/both of requirements-conda.txt or requirements-pip.txt with the package names and run the appropriate command:
* `make install-all-requirements`
* `make install-conda-requirements`
* `make install-pip-requirements`
12. (Optional) To remove the environment run `make remove-environment`
13. (Optional) To remove the kernel run `make remove-kernel`

If you don't want to use Make you can run everything manually - the commands in the `Makefile` are all recorded and most will be copy / pasteable into the terminal. Note that you will likely need to replace the environment variables with the actual values to get it to work.

## TODO
1. Implement a better logger than `loguru` - the current one produces awful error messages in Jupyter. This will improve debugging speed and can be re-implemented in other pipelines. Bonus points if it can produce a coloured output.

2. Replace the `convert_to_str()` step in the pipeline with a step that ensures [type safety](https://stackoverflow.com/questions/260626/what-is-type-safe#:~:text=Type%20safety%20means%20that%20the,%3D%201%3B%20%2F%2F%20Also%20fails.) for all columns in the pipeline, not just strings. This will improve the reliability of the pipeline and can be re-implemented in other pipelines.

3. Implement feature selection as the last step in the SKL pipeline and log the features as a parameter in MLFlow. This will allow greater experiementation and control over models and can be re-implemented in other pipelines.

4. Implementing SHAP, or a similar explainability framework. This will allow greater model transparency and make it easier to explain to stakeholders.

5. Implement Docker for easier production deployment.
